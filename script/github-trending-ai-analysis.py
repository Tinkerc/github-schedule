# coding:utf-8

import datetime
import os
import time
import codecs
import requests
import logging
import json
from pyquery import PyQuery as pq
import sys

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'prompts'))
from trending_prompts import get_batch_analysis_prompt, get_trend_summary_prompt

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class GitHubTrendingScraper:
    """GitHub Trending çˆ¬è™«"""

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:11.0) Gecko/20100101 Firefox/11.0',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Encoding': 'gzip,deflate,sdch',
            'Accept-Language': 'zh-CN,zh;q=0.8'
        }

    def scrape_trending(self, language=''):
        """
        çˆ¬å– GitHub Trending é¡¹ç›®

        Args:
            language: str, ç¼–ç¨‹è¯­è¨€ï¼ˆç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ‰€æœ‰è¯­è¨€ï¼‰

        Returns:
            list[dict]: é¡¹ç›®åˆ—è¡¨
        """
        url = f'https://github.com/trending/{language}' if language else 'https://github.com/trending'

        for attempt in range(3):
            try:
                logger.info(f"æ­£åœ¨çˆ¬å– GitHub Trending: {url}")
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()

                doc = pq(response.content)
                items = doc('div.Box article.Box-row')

                projects = []
                for item in items:
                    i = pq(item)
                    title_elem = i(".lh-condensed a")
                    if not title_elem:
                        continue

                    title = title_elem.text()
                    url_path = title_elem.attr("href")
                    description = i("p.col-9").text().strip()

                    # è·å–æ˜Ÿæ ‡æ•°
                    stars_elem = i("a[href*='/stargazers']")
                    stars_text = stars_elem.text().strip() if stars_elem else "0"

                    # è·å–ä»Šæ—¥å¢é•¿æ˜Ÿæ ‡
                    today_stars_elem = i("span.d-inline-block")
                    today_stars_text = today_stars_elem.text().strip() if today_stars_elem else "0"

                    # è·å–è¯­è¨€
                    language_elem = i("span[itemprop='programmingLanguage']")
                    language_text = language_elem.text().strip() if language_elem else "æœªçŸ¥"

                    projects.append({
                        "name": title,
                        "description": description or "æš‚æ— æè¿°",
                        "url": f"https://github.com{url_path}",
                        "stars": stars_text,
                        "language": language_text,
                        "stars_today": today_stars_text.replace("stars today", "").strip()
                    })

                logger.info(f"æˆåŠŸçˆ¬å– {len(projects)} ä¸ªé¡¹ç›®")
                return projects

            except requests.exceptions.RequestException as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/3): {str(e)}")
                if attempt < 2:
                    time.sleep(30)
                else:
                    raise

        return []

    def scrape_all_languages(self):
        """
        çˆ¬å–æ‰€æœ‰è¯­è¨€çš„ trending é¡¹ç›®ï¼ˆåªçˆ¬å–ä¸»é¡µé¢ï¼‰

        Returns:
            list[dict]: é¡¹ç›®åˆ—è¡¨ï¼ˆæœ€å¤š 25 ä¸ªï¼‰
        """
        projects = self.scrape_trending('')
        return projects[:25]  # é™åˆ¶ 25 ä¸ª


class GLMAnalyzer:
    """GLM-4.7 AI åˆ†æå™¨"""

    def __init__(self):
        self.api_key = os.environ.get('BIGMODEL_API_KEY')
        if not self.api_key:
            raise ValueError("ç¯å¢ƒå˜é‡ BIGMODEL_API_KEY æœªè®¾ç½®")

        self.api_url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
        self.headers = {
            "Accept": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    def _call_api(self, prompt):
        """
        è°ƒç”¨ GLM-4.7 API

        Args:
            prompt: str, ç”¨æˆ· prompt

        Returns:
            dict: API è¿”å›çš„ JSON æ•°æ®
        """
        payload = {
            "model": "glm-4.7",
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åˆ†æå¸ˆï¼Œè¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.7,
            "max_tokens": 8000
        }

        for attempt in range(3):
            try:
                logger.info(f"æ­£åœ¨è°ƒ GLM-4.7 API (å°è¯• {attempt + 1}/3)")
                response = requests.post(
                    self.api_url,
                    headers=self.headers,
                    json=payload,
                    timeout=60
                )
                response.raise_for_status()

                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    content = result['choices'][0]['message']['content']
                    logger.info("API è°ƒç”¨æˆåŠŸ")
                    return content
                else:
                    logger.error(f"API è¿”å›æ ¼å¼é”™è¯¯: {result}")
                    raise ValueError("Invalid API response")

            except requests.exceptions.RequestException as e:
                logger.warning(f"API è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/3): {str(e)}")
                if attempt < 2:
                    time.sleep(10)
                else:
                    raise

    def analyze_projects(self, projects):
        """
        æ‰¹é‡åˆ†æé¡¹ç›®

        Args:
            projects: list[dict], é¡¹ç›®åˆ—è¡¨

        Returns:
            dict: åˆ†æç»“æœ
        """
        prompt = get_batch_analysis_prompt(projects)
        response = self._call_api(prompt)

        # è§£æ JSON
        try:
            # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
            response = response.strip()
            if response.startswith('```json'):
                response = response[7:]
            if response.startswith('```'):
                response = response[3:]
            if response.endswith('```'):
                response = response[:-3]
            response = response.strip()

            result = json.loads(response)
            if 'projects' not in result:
                raise ValueError("Response missing 'projects' field")

            logger.info(f"æˆåŠŸåˆ†æ {len(result['projects'])} ä¸ªé¡¹ç›®")
            return result

        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æå¤±è´¥: {str(e)}")
            logger.error(f"åŸå§‹å“åº”: {response[:500]}")
            raise

    def generate_trend_summary(self, analyses):
        """
        ç”Ÿæˆè¶‹åŠ¿æ¦‚è§ˆ

        Args:
            analyses: dict, é¡¹ç›®åˆ†æç»“æœ

        Returns:
            dict: è¶‹åŠ¿åˆ†æç»“æœ
        """
        prompt = get_trend_summary_prompt(analyses)
        response = self._call_api(prompt)

        # è§£æ JSON
        try:
            response = response.strip()
            if response.startswith('```json'):
                response = response[7:]
            if response.startswith('```'):
                response = response[3:]
            if response.endswith('```'):
                response = response[:-3]
            response = response.strip()

            result = json.loads(response)
            if 'trend_overview' not in result:
                raise ValueError("Response missing 'trend_overview' field")

            logger.info("æˆåŠŸç”Ÿæˆè¶‹åŠ¿æ¦‚è§ˆ")
            return result

        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æå¤±è´¥: {str(e)}")
            logger.error(f"åŸå§‹å“åº”: {response[:500]}")
            raise


class MarkdownReportGenerator:
    """Markdown æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, output_dir):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨

        Args:
            output_dir: str, è¾“å‡ºåŸºç¡€ç›®å½•
        """
        self.output_dir = output_dir

    def generate(self, date, projects, analyses, trend_summary):
        """
        ç”Ÿæˆ Markdown æŠ¥å‘Š

        Args:
            date: str, æ—¥æœŸ (YYYY-MM-DD)
            projects: list[dict], åŸå§‹é¡¹ç›®æ•°æ®
            analyses: dict, AI åˆ†æç»“æœ
            trend_summary: dict, è¶‹åŠ¿åˆ†æç»“æœ

        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        year = date.split('-')[0]
        output_path = os.path.join(self.output_dir, 'github-trending-ai-analysis', year)
        os.makedirs(output_path, exist_ok=True)

        filename = os.path.join(output_path, f'{date}.md')

        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        content = self._generate_content(date, projects, analyses, trend_summary)

        # å†™å…¥æ–‡ä»¶
        with codecs.open(filename, 'w', 'utf-8') as f:
            f.write(content)

        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        return filename

    def _generate_content(self, date, projects, analyses, trend_summary):
        """ç”ŸæˆæŠ¥å‘Šå†…å®¹"""
        content = f"""# GitHub Trending æ¯æ—¥åˆ†ææŠ¥å‘Š - {date}

## ğŸ“ˆ ä»Šæ—¥è¶‹åŠ¿æ¦‚è§ˆ

{trend_summary.get('trend_overview', '')}

## ğŸ”¥ çƒ­é—¨é¢†åŸŸ

"""

        # çƒ­é—¨é¢†åŸŸ
        for domain in trend_summary.get('hot_domains', []):
            content += f"""### {domain.get('domain', 'æœªçŸ¥é¢†åŸŸ')}

{domain.get('reason', '')}

**ç›¸å…³é¡¹ç›®ï¼š** {', '.join(domain.get('projects', []))}

"""

        # é¡¹ç›®è¯¦æƒ…
        content += "\n## ğŸ“¦ é¡¹ç›®è¯¦æƒ…åˆ†æ\n\n"

        # åˆ›å»ºé¡¹ç›®åç§°åˆ°åˆ†æçš„æ˜ å°„
        analysis_map = {p['name']: p for p in analyses.get('projects', [])}

        for idx, project in enumerate(projects, 1):
            name = project['name']
            analysis = analysis_map.get(name, {})

            content += f"""### {idx}. [{name}]({project['url']})

**æ˜Ÿæ ‡ï¼š** {project['stars']} (ä»Šæ—¥ +{project['stars_today']}) | **è¯­è¨€ï¼š** {project['language']}

**æ ¸å¿ƒåŠŸèƒ½ï¼š** {analysis.get('core_functionality', 'æš‚æ— ')}

**é€‚ç”¨åœºæ™¯ï¼š**
{self._format_list(analysis.get('use_cases', ''))}

**æŠ€æœ¯æ ˆï¼š** {analysis.get('tech_stack', 'æœªçŸ¥')}

**æŠ€æœ¯äº®ç‚¹ï¼š**
{self._format_list(analysis.get('tech_highlights', ''))}

**å­¦ä¹ ä»·å€¼ï¼š** {analysis.get('learning_value', 'æš‚æ— ')}

---

"""

        return content

    def _format_list(self, text):
        """æ ¼å¼åŒ–åˆ—è¡¨æ–‡æœ¬"""
        if not text:
            return "- æš‚æ— "
        lines = text.strip().split('\n')
        return '\n'.join(f"- {line.strip()}" for line in lines if line.strip())


def job():
    """ä¸»ä»»åŠ¡å…¥å£"""
    start_time = time.time()
    logger.info("="*60)
    logger.info("GitHub Trending AI åˆ†æä»»åŠ¡å¼€å§‹")
    logger.info("="*60)

    try:
        # 1. è·å–å½“å‰æ—¥æœŸ
        date = datetime.datetime.now().strftime('%Y-%m-%d')
        logger.info(f"ç›®æ ‡æ—¥æœŸ: {date}")

        # 2. çˆ¬å– GitHub Trending æ•°æ®
        logger.info("æ­¥éª¤ 1/4: çˆ¬å– GitHub Trending æ•°æ®")
        scraper = GitHubTrendingScraper()
        projects = scraper.scrape_all_languages()

        if len(projects) == 0:
            logger.error("æœªçˆ¬å–åˆ°ä»»ä½•é¡¹ç›®ï¼Œä»»åŠ¡ç»ˆæ­¢")
            return

        logger.info(f"æˆåŠŸè·å– {len(projects)} ä¸ªé¡¹ç›®")

        # 3. AI æ‰¹é‡åˆ†æ
        logger.info("æ­¥éª¤ 2/4: AI æ‰¹é‡åˆ†æé¡¹ç›®")
        analyzer = GLMAnalyzer()
        analyses = analyzer.analyze_projects(projects)

        if len(analyses.get('projects', [])) == 0:
            logger.error("AI åˆ†æå¤±è´¥ï¼Œä»»åŠ¡ç»ˆæ­¢")
            return

        # 4. ç”Ÿæˆè¶‹åŠ¿æ¦‚è§ˆ
        logger.info("æ­¥éª¤ 3/4: ç”Ÿæˆè¶‹åŠ¿æ¦‚è§ˆå’Œçƒ­é—¨é¢†åŸŸ")
        trend_summary = analyzer.generate_trend_summary(analyses)

        # 5. ç”ŸæˆæŠ¥å‘Š
        logger.info("æ­¥éª¤ 4/4: ç”Ÿæˆ Markdown æŠ¥å‘Š")
        generator = MarkdownReportGenerator('output')
        report_path = generator.generate(date, projects, analyses, trend_summary)

        # 6. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        elapsed = time.time() - start_time
        logger.info("="*60)
        logger.info("ä»»åŠ¡å®Œæˆï¼")
        logger.info(f"æ‰§è¡Œæ—¶é—´: {elapsed:.2f} ç§’")
        logger.info(f"åˆ†æé¡¹ç›®æ•°: {len(projects)}")
        logger.info(f"æŠ¥å‘Šè·¯å¾„: {report_path}")
        logger.info("="*60)

    except Exception as e:
        logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    job()
